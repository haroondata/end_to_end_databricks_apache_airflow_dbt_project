# ğŸ§± End-to-End Databricks + dbt + Airflow Project

A complete data engineering pipeline using **Databricks**, **dbt**, and **Apache Airflow**.

This project demonstrates how to build and orchestrate a modern data stack with real-world practices:
- Source ingestion
- Data modeling (staging, marts)
- Reusable macros
- Automated testing
- DAG-based orchestration

---

## ğŸ“Œ Stack

- **Databricks** â€“ Lakehouse platform for storage and compute  
- **dbt (Data Build Tool)** â€“ SQL-based transformations and testing  
- **Apache Airflow** â€“ DAG scheduler for automation  

---

## ğŸš€ Features

âœ… Raw â†’ staging â†’ mart models  
âœ… Source freshness and generic tests  
âœ… Custom macros and reusable logic  
âœ… Example Airflow DAG for CI/CD  
âœ… Production-ready structure and configs  

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ dags/                # Airflow DAGs
â”œâ”€â”€ dbt/                 
â”‚   â”œâ”€â”€ models/          # dbt models (sources, staging, marts)
â”‚   â”œâ”€â”€ macros/          # Custom macros
â”‚   â”œâ”€â”€ tests/           # Generic and singular tests
â”‚   â”œâ”€â”€ dbt_project.yml  # dbt config
â”‚   â””â”€â”€ packages.yml     # dbt packages
â”œâ”€â”€ airflow/             # Airflow configs and Docker setup
â”œâ”€â”€ docker-compose.yml   # Local Airflow orchestration
â””â”€â”€ README.md
```

---

## ğŸ§ª dbt Overview

**Model Layers:**

- `models/sources/` â€“ Raw tables from source systems  
- `models/staging/` â€“ Cleaned and transformed base layer  
- `models/marts/` â€“ Business logic, facts & dimensions  

**Custom Macros & Tests:**

- Macros in `macros/` for DRY SQL  
- Generic tests (e.g., `not_null`, `unique`)  
- Singular test examples  

Run locally:

```bash
dbt run       # build models
dbt test      # run tests
```

---

## ğŸ”„ Airflow Deployment

Airflow orchestrates your dbt jobs using a DAG like:

```python
from airflow.operators.bash import BashOperator

dbt_run = BashOperator(
    task_id='dbt_run',
    bash_command='dbt run --project-dir /dbt',
)

dbt_test = BashOperator(
    task_id='dbt_test',
    bash_command='dbt test --project-dir /dbt',
)

dbt_run >> dbt_test
```

To start locally:

```bash
docker-compose up airflow-init
docker-compose up
```

---

## ğŸ“¦ Installation & Setup

### 1. Clone the Repo

```bash
git clone https://github.com/haroondata/end_to_end_databricks_apache_airflow_dbt_project.git
cd end_to_end_databricks_apache_airflow_dbt_project
```

### 2. Configure dbt Profile

In `~/.dbt/profiles.yml`:

```yaml
databricks_profile:
  target: dev
  outputs:
    dev:
      type: databricks
      host: <your-databricks-host>
      token: <your-token>
      schema: <your-schema>
      catalog: <your-catalog>
      http_path: <your-http-path>
      threads: 4
```

### 3. Start Airflow (Docker)

```bash
docker-compose up airflow-init
docker-compose up
```

---

## ğŸ“¬ Contact

Made with ğŸ’¡ by [Haroon](https://github.com/haroondata)  
Feel free to open an issue or fork this repo for your own projects!

---

## ğŸŒ Useful Tags

#dbt #databricks #apacheairflow #dataengineering #modernstack #analyticsengineering #etl #opensource
